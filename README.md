# UAV Log Viewer & AI Flight-Assistant

<p align="center">
  <img src="preview.gif" alt="UAV Log Viewer demo" width="650"/>
</p>

A modern web application for analysing and visualising UAV flight logs **with an integrated LLM-powered chat assistant**.  
Upload a `tlog` or `.bin`, explore plots, and ask natural-language questions such as *â€œWhat was the max climb rate?â€* or *â€œDo you see any GPS anomalies?â€* â€” the assistant replies instantly and proposes follow-up questions.

---

## âœ¨ Features
* ğŸ“ˆ Interactive plots for attitude, GPS, battery, RC and custom messages  
* ğŸ” Semantic search over every telemetry field (Cohere embeddings)  
* ğŸ¤– Chat assistant (Groq Llama-3) that:
  * Computes on-the-fly metrics (duration, distance, altitude, â€¦)  
  * Detects anomalies via reasoning (wind, satellite drop-outs, attitude spikes)  
  * Suggests **answerable** follow-up questions
* ğŸ— FastAPI backend, Vue 2 frontend (Webpack)  
* ğŸ³ Single-command Docker build  
* âš¡ GitHub Actions CI + rsync deploy workflow

---

## ğŸ–¼ï¸ Project Structure

```bash
UAVLogViewer
â”œâ”€ backend                     # (ADDED)
â”‚  â”œâ”€ run.py                   # local entry-point: uvicorn
â”‚  â”œâ”€ requirements.txt         # Python deps
â”‚  â”œâ”€ setup.py                 # pip-installable package
â”‚  â””â”€ uav_log_viewer           # Python package
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ core.py               # FastAPI factory
â”‚     â”œâ”€ analysis              # telemetry analytics
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ data_extractor.py
â”‚     â”‚  â”œâ”€ telemetry.py
â”‚     â”‚  â””â”€ anomalies.py
â”‚     â”œâ”€ chat                  # LLM assistant
â”‚     â”‚  â”œâ”€ __init__.py
â”‚     â”‚  â”œâ”€ prompt.py
â”‚     â”‚  â”œâ”€ processor.py
â”‚     â”‚  â””â”€ conversation.py
â”‚     â””â”€ routes                # FastAPI routers
â”‚        â”œâ”€ __init__.py
â”‚        â”œâ”€ chat.py
â”‚        â””â”€ analysis.py
â”‚
â”œâ”€ src                         # Vue 2 front-end
â”‚  â”œâ”€ components
â”‚  â”‚  â””â”€ widgets
â”‚  â”‚     â””â”€ ChatWidget.vue     # (ADDED)
â”‚  â””â”€ â€¦                        # views, router, store
â”‚
â”œâ”€ build/                      # webpack configs
â”œâ”€ config/                     # extra webpack / jest config
â”œâ”€ static/                     # assets copied verbatim
â”‚
â”œâ”€ .github
â”‚  â””â”€ workflows
â”‚     â”œâ”€ nodejs.yml            # build / test
â”‚     â”œâ”€ nodejsdeploy.yml      # rsync deploy
â”‚     â”œâ”€ nodejsdeploystable.yml
â”‚     â””â”€ release.yml           # docker release
â”‚
â”œâ”€ Dockerfile
â”œâ”€ package.json
â”œâ”€ README.md
â””â”€ preview.gif                 # animated demo

---

```


## ğŸš€ Quick Start

### 1. Clone & install

```bash
git clone https://github.com/rcaii/UAVLogViewer.git
cd UAVLogViewer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Front-end  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
npm install                        # needs Node â‰¥ 18

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Back-end  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python3 -m venv backend/venv
source backend/venv/bin/activate
pip install -r backend/requirements.txt
```

### 2. Environment variables

Create `backend/.env` and add your keys:

| Variable             | Description                                   |
|----------------------|-----------------------------------------------|
| `GROQ_API_KEY`       | API key for Groq Llama-3 (chat & anomalies)    |
| `COHERE_API_KEY`     | API key for Cohere embeddings / rerank        |
| `GROQ_API_BASE`      | _(opt)_ Custom Groq endpoint                  |
| `GROQ_DEFAULT_MODEL` | _(opt)_ Model name (defaults to llama3-70b-8192) |

**Never commit keys â€“ store them as GitHub Secrets for CI.**

### 3. Run locally

```bash
# terminal â‘  â€“ FastAPI back-end
cd backend
python run.py          # http://localhost:8000 (docs at /docs)

# terminal â‘¡ â€“ Vue front-end
npm run dev            # http://localhost:8080
```

The frontend proxies API calls to `localhost:8000` via CORS (already enabled).

### 4. Production build

```bash
npm run build          # outputs static files to dist/
```

Serve the `dist` folder behind Nginx, or build the Docker image:

```bash
docker build -t uavlogviewer .
docker run -p 8080:8080 -p 8000:8000 uavlogviewer
```

---

## âš™ï¸ Back-end endpoints

| Method | Path      | Description                      |
|--------|-----------|----------------------------------|
| GET    | `/health` | Health check (â€œokâ€)              |
| POST   | `/chat/`  | JSON `{question, telemetry?}` â†’ `{answer, suggested_questions}` |
| POST   | `/analysis/metrics` | _(example)_ compute full metric set |

Interactive docs available at `http://localhost:8000/docs` (Swagger UI).

---

## ğŸ§ª Testing

```bash
# Vue unit & e2e
npm run unit          # or npm test

# Python type check & lint (optional)
mypy backend/uav_log_viewer
pytest                # once you add test_*.py
```

---

## ğŸ¤– CI / CD

* **build.yml** â€“ Node build + unit tests  
* **nodejsdeploy.yml** â€“ rsync deploy (needs `DEPLOY_KEY`, `SERVER_IP`, `USERNAME`, `SERVER_DESTINATION` and `SERVER_PORT` GitHub Secrets)  
* **release.yml** â€“ Docker build & push (optional)



```mermaid
flowchart LR

    %% Front-end chat UI: collects question, shows reply + chips
    A[ChatWidget<br/>(Vue 3)] 

    %% REST entry point: validates JSON, hands to orchestrator
    B(FastAPI<br/>/chat)

    A -- ".tlog JSON â†’ POST" --> B

    %% Orchestrator: decides anomaly / metric / general path
    P[processor.py<br/>orchestrator] 
    B --> P

    %%  anomaly branch â€“ semantic slice â†’ LLM
    P -- anomaly? --> AN[anomaly pipeline]
    %%  metric branch â€“ slice + full-log metrics â†’ LLM
    P -- metric?  --> MT[metric pipeline]
    %%  general fallback â€“ no telem / non-UAV question
    P -- general  --> GE[general prompt]
    
    %% â”€â”€â”€â”€â”€ anomaly sub-flow â”€â”€â”€â”€â”€
    %%   semantic field selection
    DE1[data_extractor.py] 
    %%   build threshold-aware prompt
    PR1[_build_prompt()] 
    AN --> DE1 --> PR1

    %% â”€â”€â”€â”€â”€ metric sub-flow â”€â”€â”€â”€â”€
    DE2[data_extractor.py]
    TM[telemetry.py<br/>compute_metrics]
    MT --> DE2
    MT --> TM
    PR2[metric prompt builder]
    DE2 --> PR2
    TM  --> PR2

    %%  common LLM endpoint
    LLM[Groq Llama-3]
    PR1 --> LLM
    PR2 --> LLM
    GE  --> LLM
    LLM --> P

    %% conversation history buffer
    CS[(conversation_state<br/>last 15 turns)]
    P --> CS

    P --> B
    B -- "JSON {answer, suggested}" --> A
    A -- "chip click" --> B

